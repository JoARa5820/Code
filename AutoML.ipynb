{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn\n",
    "# pip install h2o\n",
    "# pip install matplotlib\n",
    "# pip install statsmodels\n",
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 라이브러리 호출 #####\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 100)  # 데이터프레임 출력 옵션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 입력값 기입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test년도 입력\n",
    "test_year = '2021'\n",
    "# y 컬럼명\n",
    "y_colnm = 'SEP_CNT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_312\"; OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07); OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n",
      "  Starting server from /home/lime/.local/lib/python3.8/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp9fu7xxv6\n",
      "  JVM stdout: /tmp/tmp9fu7xxv6/h2o_lime_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp9fu7xxv6/h2o_lime_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Asia/Seoul</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.34.0.7</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>25 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_lime_i3yjl1</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>4.271 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       Asia/Seoul\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.34.0.7\n",
       "H2O_cluster_version_age:    25 days\n",
       "H2O_cluster_name:           H2O_from_python_lime_i3yjl1\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    4.271 Gb\n",
       "H2O_cluster_total_cores:    4\n",
       "H2O_cluster_allowed_cores:  1\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.10 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## h2o 호출\n",
    "h2o.init(nthreads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 리스트 호출\n",
    "file_list = pd.DataFrame(glob.glob('data/*.csv')).rename(columns = {0:'col'})\n",
    "file_list = list(file_list['col'].apply(lambda x : x[:-4]))\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/merge_age',\n",
       " 'data/merge_age_half',\n",
       " 'data/merge_age_month',\n",
       " 'data/merge_age_quarter',\n",
       " 'data/merge_cls',\n",
       " 'data/merge_cls_half',\n",
       " 'data/merge_cls_month',\n",
       " 'data/merge_cls_quarter',\n",
       " 'data/merge_dis',\n",
       " 'data/merge_dis_half',\n",
       " 'data/merge_dis_month',\n",
       " 'data/merge_dis_quarter']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file_list = ['data/merge_dis',\n",
    "# #              'data/merge_dis_half',\n",
    "# #              'data/merge_dis_month',\n",
    "# #              'data/merge_dis_quarter']\n",
    "\n",
    "# file_list = ['data/merge_dis_month',\n",
    "#              'data/merge_dis_quarter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/merge_age\n",
      "강원 고1\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█████\n",
      "21:54:53.235: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "21:54:53.235: Skipping training of model GBM_1_AutoML_1_20220115_215451 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_1_AutoML_1_20220115_215451.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "█████████████████████\n",
      "21:54:56.256: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:54:56.256: Skipping training of model GBM_2_AutoML_1_20220115_215451 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_2_AutoML_1_20220115_215451.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:54:56.257: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:54:56.257: Skipping training of model GBM_3_AutoML_1_20220115_215451 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_3_AutoML_1_20220115_215451.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:54:56.258: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:54:56.258: Skipping training of model GBM_4_AutoML_1_20220115_215451 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_4_AutoML_1_20220115_215451.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "█████████\n",
      "21:54:57.263: StackedEnsemble_BestOfFamily_2_AutoML_1_20220115_215451 [StackedEnsemble best_of_family_2 (built with AUTO metalearner, using top model from each algorithm type)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "█████████\n",
      "21:54:58.269: StackedEnsemble_AllModels_1_AutoML_1_20220115_215451 [StackedEnsemble all_2 (built with AUTO metalearner, using all AutoML models)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "███████████████████| (done) 100%\n",
      "glm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "강원 고2\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█████\n",
      "21:55:05.26: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "21:55:05.26: Skipping training of model GBM_1_AutoML_2_20220115_215503 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_1_AutoML_2_20220115_215503.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "█████████████████████\n",
      "21:55:08.39: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:08.39: Skipping training of model GBM_2_AutoML_2_20220115_215503 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_2_AutoML_2_20220115_215503.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:55:08.39: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:08.39: Skipping training of model GBM_3_AutoML_2_20220115_215503 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_3_AutoML_2_20220115_215503.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:55:08.40: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:08.40: Skipping training of model GBM_4_AutoML_2_20220115_215503 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_4_AutoML_2_20220115_215503.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "█████████\n",
      "21:55:09.44: StackedEnsemble_BestOfFamily_2_AutoML_2_20220115_215503 [StackedEnsemble best_of_family_2 (built with AUTO metalearner, using top model from each algorithm type)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "████████\n",
      "21:55:10.49: StackedEnsemble_AllModels_1_AutoML_2_20220115_215503 [StackedEnsemble all_2 (built with AUTO metalearner, using all AutoML models)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "████████████████████| (done) 100%\n",
      "glm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "강원 고3\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█████\n",
      "21:55:16.696: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "21:55:16.696: Skipping training of model GBM_1_AutoML_3_20220115_215514 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_1_AutoML_3_20220115_215514.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "████████████████████\n",
      "21:55:19.709: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:19.709: Skipping training of model GBM_2_AutoML_3_20220115_215514 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_2_AutoML_3_20220115_215514.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:55:19.711: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:19.711: Skipping training of model GBM_3_AutoML_3_20220115_215514 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_3_AutoML_3_20220115_215514.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:55:19.713: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:19.713: Skipping training of model GBM_4_AutoML_3_20220115_215514 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_4_AutoML_3_20220115_215514.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "████████\n",
      "21:55:20.717: StackedEnsemble_BestOfFamily_2_AutoML_3_20220115_215514 [StackedEnsemble best_of_family_2 (built with AUTO metalearner, using top model from each algorithm type)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "█████████\n",
      "21:55:21.723: StackedEnsemble_AllModels_1_AutoML_3_20220115_215514 [StackedEnsemble all_2 (built with AUTO metalearner, using all AutoML models)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "█████████████████████| (done) 100%\n",
      "drf prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "강원 유3\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█████\n",
      "21:55:28.267: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "21:55:28.268: Skipping training of model GBM_1_AutoML_4_20220115_215526 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_1_AutoML_4_20220115_215526.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "█████████████████████\n",
      "21:55:31.279: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:31.279: Skipping training of model GBM_2_AutoML_4_20220115_215526 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_2_AutoML_4_20220115_215526.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:55:31.281: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:31.281: Skipping training of model GBM_3_AutoML_4_20220115_215526 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_3_AutoML_4_20220115_215526.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:55:31.282: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:31.282: Skipping training of model GBM_4_AutoML_4_20220115_215526 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_4_AutoML_4_20220115_215526.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "████████\n",
      "21:55:32.286: StackedEnsemble_BestOfFamily_2_AutoML_4_20220115_215526 [StackedEnsemble best_of_family_2 (built with AUTO metalearner, using top model from each algorithm type)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "█████████\n",
      "21:55:33.293: StackedEnsemble_AllModels_1_AutoML_4_20220115_215526 [StackedEnsemble all_2 (built with AUTO metalearner, using all AutoML models)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "████████████████████| (done) 100%\n",
      "glm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "강원 유4\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█████\n",
      "21:55:39.706: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "21:55:39.706: Skipping training of model GBM_1_AutoML_5_20220115_215537 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_1_AutoML_5_20220115_215537.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "█████████████████████\n",
      "21:55:42.719: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:42.719: Skipping training of model GBM_2_AutoML_5_20220115_215537 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_2_AutoML_5_20220115_215537.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:55:42.721: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:42.721: Skipping training of model GBM_3_AutoML_5_20220115_215537 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_3_AutoML_5_20220115_215537.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "21:55:42.723: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "21:55:42.723: Skipping training of model GBM_4_AutoML_5_20220115_215537 due to exception: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_4_AutoML_5_20220115_215537.  Details: ERRR on field: _min_rows: The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 11.0.\n",
      "\n",
      "\n",
      "█████████\n",
      "21:55:43.725: StackedEnsemble_BestOfFamily_2_AutoML_5_20220115_215537 [StackedEnsemble best_of_family_2 (built with AUTO metalearner, using top model from each algorithm type)] failed: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: Not enough data to create 5 random cross-validation splits. Either reduce nfolds, specify a larger dataset (or specify another random number seed, if applicable).\n",
      "\n",
      "█████████"
     ]
    }
   ],
   "source": [
    "for file_nm in file_list:\n",
    "\n",
    "    print(file_nm)\n",
    "    \n",
    "    ## 데이터 호출\n",
    "    tot_data = pd.read_csv(file_nm + '.csv', dtype='str', encoding = 'cp949')\n",
    "    # tot_data = tot_data.fillna(0)  # null값 처리\n",
    "\n",
    "    # ---------------------------------------------------------- #\n",
    "    # 월 데이터\n",
    "    if file_nm.find('month') != -1:\n",
    "        tot_data['STAND_TIME'] = tot_data['YEAR'] + tot_data['MONTH']\n",
    "        add_except_col = ['YEAR','MONTH','STAND_TIME']\n",
    "    # 분기 데이터\n",
    "    elif file_nm.find('quarter') != -1:\n",
    "        tot_data['STAND_TIME'] = tot_data['YEAR'] + tot_data['QUARTER']\n",
    "        add_except_col = ['YEAR','QUARTER','STAND_TIME']\n",
    "    # 반기 데이터\n",
    "    elif file_nm.find('half') != -1:\n",
    "        tot_data['STAND_TIME'] = tot_data['YEAR'] + tot_data['HALF']\n",
    "        add_except_col = ['YEAR','HALF','STAND_TIME']\n",
    "    # 연 데이터\n",
    "    else:\n",
    "        tot_data['YEAR'] = tot_data['BASE_YY']  # YEAR 컬럼 생성\n",
    "        tot_data['STAND_TIME'] = tot_data['BASE_YY']  # YEAR 컬럼 생성\n",
    "        del tot_data['BASE_YY']  # 기존 BASE_YY 컬럼 삭제\n",
    "        add_except_col = ['YEAR','STAND_TIME']\n",
    "    # ---------------------------------------------------------- #\n",
    "\n",
    "    ## 호출한 파일의 데이터 구분자 컬럼 정의(COL1 & COL2) \n",
    "    COL_LIST = list(tot_data.columns[0:3][tot_data.columns[0:3] != 'YEAR'])\n",
    "    COL1 = COL_LIST[0]\n",
    "    COL2 = COL_LIST[1]\n",
    "\n",
    "    ## train 연도 정의 : train_year (전체 기간 중 위에서 정의한 test년도 제외)\n",
    "    train_year = list(tot_data['YEAR'].unique())\n",
    "    train_year.remove(test_year)\n",
    "\n",
    "    ## 독립변수 컬럼명 정의 : x_colnm\n",
    "    tot_colnm = list(tot_data.columns)  # 전체 컬럼명\n",
    "    except_colnm = ([COL1,COL2,y_colnm] + add_except_col)  # 제외할 컬럼명\n",
    "    x_colnm = list(set(tot_colnm).difference(set(except_colnm)))  # x 컬럼명\n",
    "\n",
    "    COL1_list = list(tot_data[COL1].unique())\n",
    "    COL2_list = list(tot_data[COL2].unique())\n",
    "\n",
    "    for col1 in COL1_list:\n",
    "        for col2 in COL2_list:\n",
    "            \n",
    "            print(col1, col2)\n",
    "            \n",
    "            # ---------------------------------------------------------- #\n",
    "            # 분석 수행 데이터 정의(생성)\n",
    "            data = tot_data.loc[(tot_data[COL1] == col1) & (tot_data[COL2] == col2),].sort_values(by = 'STAND_TIME').reset_index(drop=True)\n",
    "            # ---------------------------------------------------------- #\n",
    "            # 데이터 형 변환(str -> float)\n",
    "            for chg_col in ([y_colnm] + x_colnm):\n",
    "                data[chg_col] = data[chg_col].astype('float')\n",
    "            # ---------------------------------------------------------- #\n",
    "            # train과 test로 분리\n",
    "            train = data.loc[data['YEAR'].isin(train_year),[y_colnm] + x_colnm]\n",
    "            test = data.loc[~data['YEAR'].isin(train_year),[y_colnm] + x_colnm]\n",
    "            # ---------------------------------------------------------- #\n",
    "            # x와 y로 분리\n",
    "            train_x = train[x_colnm].reset_index(drop=True)\n",
    "            train_y = train[[y_colnm]].reset_index(drop=True)\n",
    "            test_x = test[x_colnm].reset_index(drop=True)\n",
    "            test_y = test[[y_colnm]].reset_index(drop=True)\n",
    "            # ---------------------------------------------------------- #\n",
    "            # # 표준화1(StandardScaler) : 평균 = 0 / 표준편차 = 1\n",
    "            # from sklearn.preprocessing import StandardScaler\n",
    "            # scaler = StandardScaler()   \n",
    "            # std_train_x = pd.DataFrame(scaler.fit_transform(train_x), columns = list(train_x.columns))\n",
    "            # std_test_x = pd.DataFrame(scaler.transform(test_x), columns = list(test_x.columns))\n",
    "\n",
    "            # # 표준화2(Normalization) : MinMaxScaler : 최소값 0 ~ 최대값 1 : 반드시 이상치 제거 과정을 거친 후 작업해야함 \n",
    "            # from sklearn.preprocessing import MinMaxScaler\n",
    "            # scaler = MinMaxScaler()\n",
    "            # nor_std_train_x = pd.DataFrame(scaler.fit_transform(std_train_x), columns = list(train_x.columns))\n",
    "            # nor_std_test_x = pd.DataFrame(scaler.transform(std_test_x), columns = list(test_x.columns))\n",
    "\n",
    "            # 표준화3(RobustScaler) : 중앙값 = 0 / IQR(1분위(25%) ~ 3분위(75%)) = 1 : 이상치(outlier) 영향 최소화 / 더 넓게 분포\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            scaler = RobustScaler()\n",
    "            Robust_train_x = pd.DataFrame(scaler.fit_transform(train_x), columns = list(train_x.columns))\n",
    "            Robust_test_x = pd.DataFrame(scaler.transform(test_x), columns = list(test_x.columns))\n",
    "            # ---------------------------------------------------------- #\n",
    "\n",
    "            ####################### 변수 선택 과정 #######################\n",
    "\n",
    "            ## <상관분석>\n",
    "            # 상관관계는 train 데이터로만 구해야함(test 데이터 이용 X)\n",
    "            corr_data = pd.concat([train_y,Robust_train_x], axis = 1)\n",
    "            corr_rslt = corr_data.corr(method = 'pearson')  # default는 method = 'pearson'\n",
    "            corr_rslt = corr_rslt.reset_index().rename(columns = {'index':'COLNM'})\n",
    "            corr_rslt = corr_rslt.loc[corr_rslt['COLNM'] != y_colnm,]\n",
    "            corr_rslt = corr_rslt[corr_rslt[y_colnm] >= 0.5]\n",
    "\n",
    "            # x_corr = corr_rslt[['COLNM'] + list(corr_rslt['COLNM'])].set_index('COLNM')\n",
    "            # x_corr[x_corr < 0.95]\n",
    "\n",
    "            # 모델에 사용할 train, test 데이터셋\n",
    "            mdl_train_data = pd.concat([train_y, Robust_train_x], axis = 1)\n",
    "            mdl_test_data = Robust_test_x\n",
    "            \n",
    "            # 모델에 사용할 독립변수 목록\n",
    "            # (1) 상관계수로 선택\n",
    "            mdl_x_colnm = list(corr_rslt['COLNM'])\n",
    "            \n",
    "            # (2) null값이 아닌 값으로만 이루어진 경우만 선택\n",
    "            train_na_col = []\n",
    "            for col in mdl_test_data.columns:\n",
    "                if len(mdl_train_data.loc[mdl_train_data[col].isna(),]) != 0:\n",
    "                    train_na_col.append(col)\n",
    "\n",
    "            test_na_col = []\n",
    "            for col in mdl_test_data.columns:\n",
    "                if len(mdl_test_data.loc[mdl_test_data[col].isna(),]) != 0:\n",
    "                    test_na_col.append(col)\n",
    "\n",
    "            tot_na_col = list(set(train_na_col + test_na_col))\n",
    "            \n",
    "            mdl_x_colnm = list(set(mdl_x_colnm).difference(set(tot_na_col)))\n",
    "\n",
    "            # ---------------------------------------------------------- #\n",
    "    #         ## h2o 호출\n",
    "    #         h2o.init(nthreads=1)\n",
    "            # ---------------------------------------------------------- #\n",
    "            ## h2o 데이터프레임 형식으로 변환\n",
    "            h2o_train_data = h2o.H2OFrame(mdl_train_data)\n",
    "            h2o_test_data = h2o.H2OFrame(mdl_test_data)\n",
    "\n",
    "            ## 모델 생성\n",
    "    #         start_time = time.time()\n",
    "            model = H2OAutoML(max_models=20, max_runtime_secs=10, seed=1234)\n",
    "            model.train(x = mdl_x_colnm, y = y_colnm,\n",
    "                        training_frame = h2o_train_data)  # x : 독립변수 / y : 종속변수 / training_frame : 학습데이터 / 모델 검증은 pass\n",
    "    #         print('모델 생성 시간 : ', time.time() - start_time)\n",
    "            # --------------------------------------------------------------- #\n",
    "            # # View the AutoML Leaderboard\n",
    "            # lb = model.leaderboard\n",
    "            # lb.head(rows = 10)  # 가장 성능 좋은 모델 top 10개 확인\n",
    "            # model.leader  # 리더보드 값 확인 : The leader model is stored here\n",
    "\n",
    "            # ## 모델 조사\n",
    "            # m = model.leader  # Get the best model using the metric\n",
    "            # m = model.get_best_model()  # this is equivalent to\n",
    "\n",
    "            ## AutoML 출력\n",
    "            # Get leaderboard with all possible columns\n",
    "            lb = h2o.automl.get_leaderboard(model, extra_columns = \"ALL\")  # lb : top 10개 모델에 대한 리더보드 확인\n",
    "            save_lb = lb.as_data_frame()  # pandas 데이터프레임으로 형변환\n",
    "            # --------------------------------------------------------------- #\n",
    "            ## 예측 수행\n",
    "            pred = model.predict(h2o_test_data)\n",
    "\n",
    "            ## h2o 데이터프레임을 pandas 데이터프레임으로 변환\n",
    "            pred = h2o.as_list(pred, use_pandas=True)  # 또는 pred.as_data_frame()\n",
    "            pred.rename(columns={'predict':'PREDICT'}, inplace=True)\n",
    "            # --------------------------------------------------------------------------------------- #\n",
    "    #         ## h2o 종료\n",
    "    #         h2o.cluster().shutdown()\n",
    "            # ---------------------------------------------------------- #\n",
    "\n",
    "            ## 결과값 정리\n",
    "            rslt = pd.concat([pred, test_y], axis = 1)\n",
    "            rslt['DIFF'] = rslt['PREDICT'] - rslt['SEP_CNT']\n",
    "            rslt['target'] = (col1 + '_' + col2)\n",
    "            rslt['mdl_x_colnm'] = str(mdl_x_colnm)\n",
    "            rslt['BEST_MDL'] = save_lb['model_id'][0]\n",
    "            rslt['MSE'] = (rslt['DIFF']**2)\n",
    "            rslt['MSE'] = round(rslt['MSE'].mean(),4)\n",
    "            rslt['stand_time'] = list(data.loc[~data['YEAR'].isin(train_year),'STAND_TIME'])\n",
    "            rslt = rslt[['target', 'stand_time', 'PREDICT', 'SEP_CNT', 'MSE', 'BEST_MDL']]\n",
    "\n",
    "            ## 결과값 저장\n",
    "            if (col1 == COL1_list[0]) & (col2 == COL2_list[0]):\n",
    "                col1_col2_rslt = rslt\n",
    "            else:\n",
    "                col1_col2_rslt = col1_col2_rslt.append(rslt)\n",
    "\n",
    "    col1_col2_rslt.to_csv('result/result_' + file_nm.split('/')[1] + '.csv', index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('총 모델 생성 시간 : ', time.time() - total_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 리스트 호출\n",
    "file_list = pd.DataFrame(glob.glob('data/*.csv')).rename(columns = {0:'col'})\n",
    "file_list = list(file_list['col'].apply(lambda x : x[:-4]))\n",
    "file_list.sort()\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/merge_age' + '.csv', dtype='str', encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/merge_cls' + '.csv', dtype='str', encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/merge_dis' + '.csv', dtype='str', encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/merge_cls_month' + '.csv', dtype='str', encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_nm = file_list[0]\n",
    "file_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_nm.split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_nm in file_list:\n",
    "    tmp = pd.read_csv('result/result_' + file_nm.split('/')[1] + '.csv')\n",
    "    globals()[file_nm.split('/')[1]] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_age_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_age_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_age_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cls_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cls_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cls_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dis_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dis_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dis_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
